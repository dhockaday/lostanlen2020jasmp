
\documentclass{bmcart}

\usepackage{amsthm,amsmath}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{color}
\usepackage{xspace}
\usepackage{graphicx}
% \def\includegraphic[#1]{}
% \def\includegraphics[#1]{}


%%% Put your definitions there:
\startlocaldefs

\newcommand*{\eg}{e.g.\@\xspace}
\newcommand*{\ie}{i.e.\@\xspace}
\newcommand{\ipt}{IPT\xspace}
\newcommand{\ipts}{IPTs\xspace}

\newcommand{\patk}{p\mathrm{@}k}

\newcommand{\ml}[1]{\textcolor{blue}{ML: #1}}
\newcommand{\vl}[1]{\textcolor{red}{VL: #1}}
\newcommand{\ja}[1]{\textcolor{purple}{JA: #1}}

\endlocaldefs


%%% Begin ...
\begin{document}

%%% Start of article front matter
\begin{frontmatter}

\begin{fmbox}
\dochead{Research}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the title of your article here     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Learning Auditory Similarities Between Instrumental Playing Techniques}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors here                   %%
%%                                          %%
%% Specify information, if available,       %%
%% in the form:                             %%
%%   <key>={<id1>,<id2>}                    %%
%%   <key>=                                 %%
%% Comment or delete the keys which are     %%
%% not used. Repeat \author command as much %%
%% as required.                             %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author[
   addressref={aff1},                   % id's of addresses, e.g. {aff1,aff2}
   corref={aff1},                       % id of corresponding address, if any
   noteref={n1},                        % id's of article notes, if any
   email={email@example.com}            % email address
]{\inits{VL}\fnm{Vincent} \snm{Lostanlen}}
\author[
   addressref={aff2},
   email={email@example.com}
]{\inits{CE}\fnm{Christian} \snm{El-Hajj}}
\author[
   addressref={aff2},
   email={email@example.com}
]{\inits{MR}\fnm{Mathias} \snm{Rossignol}}
\author[
   addressref={aff2},
   email={email@example.com}
]{\inits{GL}\fnm{Gr\'egoire} \snm{Lafay}}
\author[
   addressref={ccm},
   email={janden@flatironinstitute.org}
]{\inits{JA}\fnm{Joakim} \snm{And\'en}}
\author[
   addressref={aff2},
   email={email@example.com}
]{\inits{ML}\fnm{Mathieu} \snm{Lagrange}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors' addresses here        %%
%%                                          %%
%% Repeat \address commands as much as      %%
%% required.                                %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\address[id=aff1]{%                           % unique id
  \orgname{Department of Zoology, Cambridge}, % university, etc
  \street{Waterloo Road},                     %
  %\postcode{}                                % post or zip code
  \city{London},                              % city
  \cny{UK}                                    % country
}
\address[id=aff2]{%
  \orgname{Marine Ecology Department, Institute of Marine Sciences Kiel},
  \street{D\"{u}sternbrooker Weg 20},
  \postcode{24105}
  \city{Kiel},
  \cny{Germany}
}
\address[id=ccm]{%
  \orgname{Center for Computational Mathematics, Flatiron Institute},
  \street{162 5th Avenue},
  \city{New York, NY 10010},
  \cny{USA}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter short notes here                   %%
%%                                          %%
%% Short notes will be after addresses      %%
%% on first page.                           %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{artnotes}
\note[id=n1]{Equal contributor}
\end{artnotes}

\end{fmbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The Abstract begins here                 %%
%%                                          %%
%% Please refer to the Instructions for     %%
%% authors on http://www.biomedcentral.com  %%
%% and include the section headings         %%
%% accordingly for your article type.       %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstractbox}

\begin{abstract}
Abstract here.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The keywords begin here                  %%
%%                                          %%
%% Put each keyword in separate \kwd{}.     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{keyword}
\kwd{sample}
\kwd{article}
\kwd{author}
\end{keyword}

\end{abstractbox}

\end{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The Main Body begins here                %%
%%                                          %%
%% Please refer to the instructions for     %%
%% authors on:                              %%
%% http://www.biomedcentral.com/info/authors%%
%% and include the section headings         %%
%% accordingly for your article type.       %%
%%                                          %%
%% See the Results and Discussion section   %%
%% for details on how to create sub-sections%%
%%                                          %%
%% use \cite{...} to cite references        %%
%%  \cite{koon} and                         %%
%%  \cite{oreg,khar,zvai,xjon,schn,pond}    %%
%%  \nocite{smith,marg,hunn,advi,koha,mouse}%%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Introduction}
\label{sec:intro}

Music information retrieval operates at either of two levels: symbolic and auditory.
By resting on a notation system, the symbolic level facilitates the comparison of musical notes in terms of quantitative attributes, such as duration, pitch, or intensity at the source.
Timbre, in contrast, is a qualitative attribute of music, thus irreducible to a one-dimensional axis.
As a result, symbolic representations describe timbre indirectly, either via visuotactile metaphors (\eg{} dry, rough, bright, round, and so forth) or by specifying an instrumental playing technique (\eg{} bowed or plucked).

Despite their widespread use, linguistic references to musical timbre suffer from limited adequacy.
On the one hand, adjectives such as \emph{bright} or \emph{rough} are prone to misunderstanding, as they do not prescribe any musical gesture that is capable of achieving them.
On the other hand, the sole mention of playing technique does not determine any intended effect; and therefore, fails to generalize to the entire instrumentarium.
For instance, although the term \emph{breathy} alludes to a playing technique that is peculiar to wind instruments, a cellist may accomplish a seemingly breathy timbre by bowing near the fingerboard, i.e., \emph{sul tasto} in the classical terminology.

The prospect of modeling timbre perception thus necessarily exceeds the symbolic domain.
Instead, it involves a cognitive process which arises from the subjective experience of listening.
From an audio signal processing standpoint, the task of simulating this cognitive process amounts to the design of a multidimensional feature space wherein some distance function evaluates arbitrary pairs of stimuli.
Rather than to merely discriminate instruments as mutually exclusive categories, this distance function must reflect a consensus of quantitative judgments of acoustic dissimilarity, all other parameters --- \ie{} duration, pitch, and intensity --- being equal.

Behind the overarching challenge of coming up with a robust predictive challenge for human behavior, the main practical application of timbre similarity retrieval lies in the emerging topic of computer-assisted instrumentation \ml{vl : cite esling and others ?}.
Following the aesthetic tradition of spectralism in contemporary music creation, the composer queries the machine with an arbitrary audio signal.
The outcome of this is an instrumental audio sample, alongside textual attributes of playing technique.
In this context, the computer serves as a bridge from the auditory level to the symbolic level, allowing to approximate a potentially infinite realm of timbral sensations by an orchestra of priorly specified extent and technical range.
To maximize the perceptual fidelity of such an approximation, a query-by-example search engine in the auditory domain should summarize the vastest possible vocabulary of timbral templates.

In this paper, we propose a machine listening model which computes the dissimilarity in timbre between any two audio samples.

\ml{decrire les contributions signal ici}

The main originality of our model is that it encompasses a broad range of extended playing techniques, well beyond the so-called ``ordinary'' mode of acoustic production \ml{vl cite dlfm ?}.
Specifically, we fit pairwise judgments for 78 different techniques arising from 16 instruments, some of which include removable timbre-altering devices such as mutes.
A second originality of our model is that we purposefully disregard the playing technique metadata underlying each audio sample during the training phase of our model.
Said otherwise, we rely on listeners, not performers, to define and evaluate the task at hand.

%% TODO: list Sections

%% TODO: mention companion website and open-source software


\section*{Related work}
Timbre involves multiple time scales in conjunction, from a few microseconds for an attack transient to several seconds for a sustained tone.
Therefore, computational models of timbre perception must summarize acoustic information over a long analysis window, typically amounting to $10^{4}$ digital audio samples or more.
Mapping this input to a feature space in which distances denote timbral dissimilarities requires a data-driven stage of dimensionality reduction.
The scientific literature exhibits a methodological divide as regards the collection of human-annotated data.

On the one hand, most publications in music information retrieval cast timbre modeling as an audio classification problem  \ml{vl cite}.
In this context, the instrumentation of each musical excerpt serves as an unstructured set of ``audio tags'', encoded as binary outputs within some predefined label space.
Because such tags often belong to the publicly available metadata of music releases, the process of curating a training set for musical instrument classification requires little or no human intervention.
Although scraping user-generated content from online music hosting platforms may not always reflect the true instrumentation with perfect accuracy, it offers a scalable and ecologically valid insight onto the acoustic underpinnings of musical timbre.
Furthermore, supplementing user-generated content with the outcome of a crowdsourced annotation campaign  allows an explicit verification of instrument tags.
For instance, the Open-MIC dataset, maintained by the Community for Open and Sustainable Music Information Research (COSMIR), comprises a vast corpus of X polyphonic music recordings spanning X instruments, as a derivative of the Free Music Archive (FMA) dataset.
Another example is the Medley-solos-DB dataset, which comprises X monophonic excerpts from 8 instruments, as a derivative of the MedleyDB dataset of multitrack music.
Over the past decade, the surge of these large digital audio collections, together with the democratization of high-performance computing on dedicated hardware, has spurred the development of deep learning techniques, and notably deep convolutional networks, in music instrument recognition.
We refer to X for a recent review of the state of the art in this domain.

On the other hand, the field of music psychology investigates timbre with the aim of discovering its physiological and behavioral foundations \ml{vl cite}.
In this setting, prior knowledge, however accurate, of instrumentation does not suffice to conduct a study.
Rather, each excerpt must be played back to multiple human listeners.
Yet, collecting subjective responses to acoustic stimuli is a tedious and unscalable procedure, thus restricting the size of the musical corpus under study.
Such restriction hampers the applicability of iterative representation learning algorithms, such as stochastic gradient descent in artificial neural networks.
Nevertheless, advanced methods in electrophysiology allow to observe the firing patterns of biological neurons in the presence of controlled stimuli.
This observation, originally carried out on the ferret, has led to a comprehensive mapping of the primary auditory cortex, in terms of its spectrotemporal receptive fields (STRF) \ml{vl citer, dire que c'est du coup une base solide de représentation et que le modèle mathamtique que l'on utilisera ici est quasi equivalent}.

\ml{parler des travaux de Mounia}


\section*{Timbre similarity judgments}
\label{sec:survey}

To construct a dataset for exploring the perception of timbre, we conducted a survey on the perceived similarity of a set of sounds.
The set consists of $78$ recordings of different instrument--PT (IPT) pairs, each played at the same pitch (C4) and nuance \textit{mezzo-forte}. For more information on the dataset considered in this study, see Section \ref{sec:dataset}.
In the survey, the subject is asked to organize dots on a two-dimensional place in a free sorting task.
Each dot corresponded to a different IPT recording and is moved by the user dragging them with the mouse.
In addition to spatially arranging the dots, the user is also asked to assign one of $20$ colors to each dot, providing a simple information of clustering of the sounds.
While both the location and color of each dot was recorded, only the color is used in our subsequent analysis---the spatial arrangement is only used to facilitate the user's ability to organize the sounds.
More information on the data collection process is available in  Section \ref{sec:judgments}.

The above survey was implemented as an interactive website, links to which were sent out on internal mailing lists of the \emph{Conservatoire national sup\'erieur de musique et de danse de Paris} (CNSMDP) and international mailing lists focusing on research in audio and music processing.
We made the experiment available for two months, during which we obtained results for a total of $31$ subjects.
The final arrangement and color assignment of one of the subjects is shown in Figure \ref{fix:xp2display}.

\begin{figure}
\center
\includegraphics[width = \textwidth]{figures/xp2example1.png}
\caption{Spatial and group organization provided by subject 1.}
\label{fig:xp2display}
\end{figure}

For each subject, a matrix of size $78 \times 78$ is constructed to represent the similarity judgments for each pair of the $78$ IPTs.
Specifically, the $(i, j)$th element of the matrix was set to one if the $i$th and $j$th IPT were assigned the same color.
If not, it is set to zero.
In other words, the clustering assignment of each subject as a binary similarity matrix. Depending on the application, it may be of interest to condition the timbre space using the information of only one subject. In this case, the corresponding binary similarity matrix may be used and the resulting timbre space is specialized to a given person.


It may also be of interest to condition the space using the aggregate similarity judgments given by several subjects. We found that using a hypergraph partitioning algorithm \cite{kernighan1970efficient,han1997scalable,strehl2002cluster} to produce the consensus similarity matrix performed well. Interestingly, although several sounds pairs were obtained from the same instrument (but different PTs) and others were obtained using the same PT (but different instruments), the consensus clustering did not favor one of these taxonomies over the other, see Figure \ref{fig:consensusVsIpt}.

\begin{figure}
\center
\includegraphics[width = \textwidth]{../code/xp2/consensusVsI.png}
\caption{'ASax'    'BBTb'    'BbCl'    'Bn'    'Cb'    'Fl'    'Gtr'    'Hn'    'Hp'    'Ob'    'TTbn'    'TpC'  'Va'    'Vc'}
\label{fig:consensusVsIpt}
\end{figure}

\begin{figure}
\center
\includegraphics[width = \textwidth]{../code/xp2/consensusVsPt.png}
\caption{ 'art-harm'    'blow-no-reed'    'explo-slap'    'harm-fngr'    'key-cl'    'legno-batt'    'legno-tratto'
    'nonvib'    'ord'    'ord-closed'    'ord-hi-reg'    'ord-open'    'pdl-tone'    'pizz'    'pizz-bartok'
    'pizz-lv'    'pizz-sec'    'pont'    'slap'    'slap-unp'    'tasto'    'tng-ram'}
\label{fig:consensusVsIpt}
\end{figure}

\ja{We should have some figure to back this up.
Can this be done using the tag clouds?}
\ml{tentative solution using similarity matrices, see fig, if ok should be improved by vl (code/xp2/consensusVsIpt.m)}

\section*{Proposed approach}
\label{sec:method}

\ml{introduce the system globally, while stressing the ability of the system to be conditionned for one or many people}

\subsection*{Scattering transform}
\label{sec:scattering}

While cortical representations have been successful in distinguishing properties of audio recordings (including similarity analysis and classification tasks), they remain models for the electrophysiological signals recorded in the audio cortex.
As such, their calculation is not always computationally efficient and their mathematical properties may be difficult to analyze directly.

A closely related representation is that of the time scattering transform, introduced by And\'{e}n and Mallat \cite{anden2011multiscale,anden2014deep}.
Given an audio recording, it decomposes the signal using an analytic wavelet transform and computes the modulus, obtaining the \emph{scalogram} of the signal.
The scalogram is similar to the spectrogram, but has a time--frequency resolution that varies with frequency: at low frequency, the filters are narrowly tuned in frequency (and hence wide in time), while at high frequency, the filters are wide in frequency (and hence narrowly concentrated in time).
This representation contains a large amount of information, but not in a form suitable for building statistical models.
In particular, the scalogram is not \emph{stable} in the sense that small deformations to the original signal may induce large changes to the scalogram.
It also lacks \emph{invariance} to time-shifting, which does not affect our perception of a sound, and should therefore not affect its timbre model.
One way to resolve this is to average the scalogram in time to obtain a spectral profile of the signal, yielding the first-order scattering coefficients.
These possess both invariance to time-shifting and stability to time-warping deformations \cite{anden2014deep}.

Since these first-order coefficients only include spectral information, they cannot accurately characterize more sophisticated structure in the signal.
One way to augment them is to calculate another wavelet decomposition in time, this time on each frequency channel of the scalogram.
Again, we compute the modulus of the wavelet coefficients and average in time.
The resulting coefficients are known as the second-order time scattering coefficients.
These also possess the desired invariance and stability properties, but capture more information on the temporal structure of the scalogram \cite{anden2014deep}.

While richer than a simple spectral decomposition, such as MFCCs, the second-order time scattering coefficients described above primarily capture structure in time.
Indeed, they do not explicitly encode frequency structure in an invariant and stable manner.
If a signal is transposed or warped in frequency by a small amount, we do not expect its timbre to change significantly.
We would therefore like our representation to have the same invariance and stability properties along the frequency axis.
This is achieved by taking first- and second-order scattering coefficients and computing a second scattering transform, this time along the log-frequency axis.
The resulting coefficients are known as separable time and frequency scattering coefficients \cite{anden2014deep}.

This representation does possess the desired invariance and stability properties, but its separable nature limits its power to adequately represent time--frequency structures that do not separate cleanly in time and frequency, such as chirps.
To remedy this, another approach was suggested by And\'{e}n et al. \cite{anden2015joint,anden2019joint} wherein the temporal wavelet decomposition of the scalogram is replaced with a two-dimensional, spectro-temporal decomposition.
Similarly to the cortical representation, the result is a four-dimensional array indexed by time, acoustic frequency, modulation frequency (or \emph{rate}), and spectral modulation frequency, known as \emph{quefrency} (or \emph{scale}).
As before, we compute the modulus and average in time, obtaining a set of coefficients known as second-order joint time--frequency scattering coefficients.

The definition of the scattering transform in terms of wavelet decompositions and modulus nonlinearities presents several advantages.
On the computational side, the critical bandwidth guarantees of wavelet transforms allows us to judiciously subsample the output, resulting in a lower-dimensional representation compared to the cortical representations.
The scattering transform also satisfies the aforementioned stability conditions, providing a guarantee that slightly deforming a signal only results in a negligible change in its scattering transform.
Furthermore, we may calculate the scattering tranforms of several model signals, including amplitude-modulated harmonic sounds and noise excitations \cite{anden2012scattering,anden2014deep}, frequency-modulated sinusoids (e.g., chirps) \cite{anden2012scattering,anden2019joint}, and beating phenomena \cite{anden2014deep}.
They have also enjoyed success in several audio classification and similarity retrieval tasks \cite{anden2011multiscale,bauge2013representing,anden2014deep,anden2019joint,lostanlen2018relevance,lostanlen2018extended}.

\subsection*{Similarity weighting}
\label{sec:weighting}

The standard Euclidean distance on the raw scattering coefficients  provides some sort universal representation of timbre dissimilarity. Considering some sort of learning to apply further transformations to the scattering coefficients and compute the Euclidean distance on the result.

Due to the size of corpus, we restrict ourselves to a simple linear transform of the scattering coefficients. More flexible and powerful methods could be brought to bear on this problem, such as deep neural networks, but with their flexibility comes an increase of the needed size of training data.

Specifically, we construct a positive semidefinite matrix $L$ that we apply to the scattering coefficients $x$ so that the Euclidean distance between $Lx$ and $Ly$ best approximates the consensus clustering obtained above.
We expect the resulting representation $Lx$ to provide a more accurate model for timbre perception.

We now use the consensus clustering to reweight the similarity between feature vectors (either MFCCs or scattering transforms).
For this, we use the \emph{large margin nearest neighbor} (LMNN) metric learning algorithm \cite{weinberger2006distance, weinberger2009distance}.
This approach constructs a positive semi-definite weighting matrix $L$ such that the distance $\|Lx - Ly\|$ best classifies a set of points into a fixed clustering assignment.

\section*{Results}
\label{sec:results}

The processing pipeline presented in Figure \ref{fig:pipeline} addresses the problem of conditioning a timbre space using perceptual judgments of timbral similarity. The relevance of the many design choices that had to be made  are addressed in this section using an experimental protocol that considers a dataset of audio that will be given as input to the processing pipeline and one metric used for evaluating its performance under a given experimental setting.

\subsection*{Data}

The perceptual data gathered during the perceptual experiment considers 78 \ipts each represented by one sound recording that the subject could listen to before clustering the \ipts. For those 78 \ipts, 31 clusterings $C_k, 0<k<31, Card(C_k)=78$ are available, that are our reference in terms of timbre similarity. If \ipt $i$ and \ipt $j$ are clustered together by subject $k$, then $C_k(i)=C_k(j)$. In essence, an effective ranking system should retrieve sounds that belong to \ipts that are clustered together in each of the 31 clusterings. Depending on the preferences of subjects, there may be competition between clusterings that should be mitigated when appreciating quantitatively the ranking performance.

In order to provide enough data for the numerical analysis described in this section, we assume that the impact of the pitch and nuance on perceptual similarity is much lower than the timbral similarity constraints expressed by the perceptual judgments of the subjects. Assuming invariance to timbre with respect to pitch and nuance is in line with the ansi definition of timbre \ml{vl, citation}, it is  thus taken in this paper as a reasonable assumption. Quantification of this invariance is left for further study.

The whole range of nuance and pitch is thus considered for each of the 78 \ipts, leading to a dataset composed of 9346 sound recordings. The clusterings $C_k$ are propagated to this dataset to produce new reference clusterings $D_k, 0<k<31, Card(D_k)=9346$ using the following scheme. If a sound of this dataset $s_a$ corresponds to \ipt $i$, then $D_k(a) = C_k(i), \forall k$.

\subsection*{Metric}

The metric considered in this paper is the average precision at rank $k=5$ (ap@5). It allows us to evaluate if the outputs of the proposed system are likely to satisfy the user. The ap@5 is computed as follows.

Considering an output of the system, that is a features vector $F_n$ per sound $s_n$, the precision at rank $5$ is computed for each of the $D_k$ clusterings, and the $31$ p@5 are averaged to obtain the ap@5.

For a given reference clustering $D_k$, the p@5 is computed as follows. For each sound $s_q, 0<q<9346$ considered as a query, its 5 nearest neighbors $s_n, 0<n<5$ in the timbre space under evaluation are retrieved using the Euclidean distance between corresponding features vectors. For this query, the precision is defined as:
$$
p@5_q = \frac{\sum_{n=5}^5 D_q==D_n}{5}
$$
The p@5 is the average $p@5_q$ computed over all queries.

\ml{note pour vincent, on a pas de fold, mais on a les resultats par sujet, on peut donc evaluer la dispersion de la p@5 par sujets si cela est utile.}

The choice of the rank is chosen with the application in mind. When searching for sounds that are similar to a query, the listener is likely to listen to up to $5$ proposals, rarely more. For the same reason, more global metrics commonly considered for ranking tasks such as the mean average precision is not considered here.

\subsection*{Analysis}

The processing pipeline is composed of 2 main building blocks. The first block $B_1$ takes a digital audio recording as input and outputs one vector of features. For this block, the main factors are the time support $B_1^t$ for computing the features and the type of features $B_1^f$. We considered $B_1^t \in \{ 25, 128, 250, 500, 1000 \}$ ms and $B_1^f \in \{ \text{mfccs}, \text{separable scattering}, \text{joint scattering} \}$. The mfccs are the standard Mel frequency cepstral coefficients with 40 Mel bands and selection of the 13$^\text{th}$ first discrete cosine transform (dct) coefficients.

\ml{provide parameters for scattering
\texttt{
case 'scat'
    filt\_opt.Q = [12 1];
    filt\_opt.J = T\_to\_J(sr*setting.sct/1000, filt\_opt);
case 'tfscat'
    tm\_filt\_opt.Q = [12 1];
    tm\_filt\_opt.J = T\_to\_J(sr*setting.sct/1000, tm\_filt\_opt);
    fr\_filt\_opt.J = 4;}}

As found in many studies, log compression and median processing are beneficial post processing steps for the scattering.

For each sound, a set of features are computed for each time support. This set is averaged over time to give one feature vector per sound.
All features are standardized with zero mean and unit variance across the dataset.

The second block $B_2$ considers one vector of features and the consensus clustering $D_c$ as inputs and another vector of features as output. For this block, the main factor is the type of projection $B_2^p \in \{ \text{lda}, \text{lmnn} \}$. The lda is the linear discriminant analysis \cite{} and the lmnn is the large margin nearest neighbors algorithm presented in Section \ref{}. For this algorithm the neighborhood is set to $5$.

The complete experiments with all settings have been computed. For the sake of clarity, only part of the results are discussed here.

The best performing system considers the separable scattering computed over $1000$ms time support, projected using the lmnn algorithm. Evaluated over the entire dataset, that is the computation of the projection and the evaluation are performed on the whole dataset, this system reaches the satifactory performance of $99.5\%$ of $ap@5$. That means that, for a given sound dataset of \ipts for which subjective evaluation of similarity between \ipts are available, a very large number of queries would return interesting results to the user using this processing pipeline.

In order to better understand the benefits of each design choice, an ablation study \ml{vl ref ??} is now performed using a slightly different protocol. In order to reduce the risk of misleading performance results using learning systems due to overfitting, it common to identify factors of variability that could lead to such problems and mitigate those. In our case, two factors can readily be identified, the variability of the audio data and the one of the perceptual judgments.

We mitigate the first by considering a random split of the dataset in two halves, one for learning the projection and one for evaluating the performance. The impact of the variability of the perceptual judgments and its impact on machine listening systems such as the one discussed in this paper is not in the scope of this paper and left for further research. By considering this protocol, the proposed system reaches a $ap@5$ of $95.5\%$. If considering the same first half both for learning and evaluation leads to a $ap@5$ of $96.5\%$, indicating a low risk of overfitting.

Another risk when considering reprojection techniques such as the lmnn may appear when the dimensionality of the features gets closer to the  number of items in the dataset. For example there exists a trivial solution for optimizing the p@1 when the dimensionality of the features is equal to the number of items in the dataset. In this case, no information needs to be carried by the features set to obtain perfect performance.

In order to quantify the importance of the information carried by the feature set in the final performance, the numerical values of the features are replaced with random numbers sampled from a uniform distribution. In this case, the performance drops down to $23.6\%$. Not considering the learning block gives a $aP@5$ of $23.8\%$. It can be concluded from these results that the learning block is not able to perform effectively if no information is carried by the input features set.

The slight decrease of performance when considering lmnn may be explained by the fact that the learning algorithm optimizes the projection using correlations in the training set that are not found in the testing set.

The lmnn algorithm is a popular algorithm for solving such a task, but other alternatives can be considered. One of those is the linear discriminant analysis \ml{ja ref ?}\cite{}. When considering the lda in place of the lmnn algorithm, the performance drops down to $81.2\%$.

The lower performance of the lda may be explained by the fact that the dimensionality of the projected feature set is set to one plus the number of clusters in the reference clustering, which lowers the dimensionality from 1180 to 20.

Considering a learning block implies some constraints. Performing the learning on large datasets require some computation. The projection has to be updated if the reference clustering or the dataset are modified significantly. Not considering any learning block reduces the performance to $92.9\% \pm 3$.

Concerning the feature computation block $B_1$, an important factor is the time support. When considering the commonly used duration of 25ms, the performance drop down to $92\% \pm$. This demonstrates that the joint scattering is able to benefit from a larger time support, possibly to capture long term modulations which plays an important part when considering extended playing techniques.

Considering the separable scattering gives a performance of $93\% \pm 4.2$. \ml{ ja write details on what is lost here ?}

Simplifying further the feature set by considering the mfccs gives a performance of $82.3\% \pm 7.1$. Considering the mfccs is equivalent to considering the separable scattering at order 1. Compared to the separable scattering, the information captured by the mfccs is considerably reduced. In order to confirm that the additional information captured by the joint scattering is effectively important, another feature set is considered where the mfccs are complemented with monomials in order to reach the dimensionality of the joint scattering. To do so, the 40 features $f_i$ of the mfccs are multiplied together $f_{i, j} = f_i*f_j, i \neq j$. The resulting features set is composed of the 40 mfccs features $f_i$ and the first 1140 $f_{i, j}$. The resulting aP@5 of $81.8\% \pm 7$ indicates that the increase of dimensionality is likely to have no effect, confirming the relevance of the additional information provided by the joint scattering. \ml{vl outro on modulations}

\section*{Discussion}
\label{sec:discussion}

\ml{ parler des contributions signal ici}

The success of the above system in identifying timbral similarities has immediate applications in browsing music databases.
These are typically organized based on instrumental and playing techniques taxonomies, with additional keywords offering a more flexible organization.
Accessing the sounds in these databases therefore requires some knowledge of the taxonomy and keywords used.
Furthermore, the user needs to have some idea of the particular IPT they are searching for.

As an alternative, content-based searches allow the user to identify sounds based on similarity to some given query sound.
This query-by-example approach provides an opportunity to search for sounds without having a specific instrument of PT in mind, yielding a wider range of available sounds.
A composer with access to such a system would therefore be able to draw on a more diverse palette of musical timbre.

The computational model proposed in this work is well suited to such a query-by-example task.
We have shown that it is able to approximate well the timbral judgments of a wide range of subjects included in our study.
Not only that, but the system can be easily retrained to approximate an individual user's timbre perception by having that user perform the clustering task on the reduced set of $78$ IPTs and running the LMNN training step on those clustering assignments.
This model can then be applied to new data, or, alternatively, be retrained with these new examples if the existing model proves unsatisfactory.

We shall note, however, that the current model has several drawbacks.
First, it is only applied to instrumental sounds.
While this has the advantage of simplifying the interpretation of the results, the range of timbre under consideration is necessarily limited (although less restricted than only considering \emph{ordinario} PTs).
This also makes applications such as \emph{query-by-humming} difficult, since we cannot guarantee that the timbral similarity measure is accurate for such sounds.

\ml{vl je reecrira le par du dessus en positif fture work}

That being said, the above model is general enough to encompass a wide variety of recordings, not just instrumental sounds.
Indeed, the tools used (scattering transforms and LMNN weighting matrices) do not depend on the type of sound being processed and work for more general classes of sounds.
To extend the model, it is only necessary to retrain the LMNN weighting matrix by supplying it with new cluster assignments.
These can again be obtained by performing a new clustering experiment with one or more human subjects.

Another aspect is the granularity of the similarity judgments.
In the above method, we have used \emph{hard} clustering assignments to build our model.
A more nuanced similarity judgment would ask users to rate the similarity of a pair of IPTs on a more graduated scale, which would yield a finer, or \emph{soft}, assignment.
This however, comes with additional difficulties in providing a consistent scale across subjects, but could be feasible if the goal is to only adapt the timbral space to a single individual.
An approach not based on clustering would also have to replace the LMNN algorithm with one that accounts for such soft assigments.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Backmatter begins here                   %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{backmatter}

\section*{Competing interests}
  The authors declare that they have no competing interests.

\section*{Author's contributions}
    Text for this section \ldots

\section*{Acknowledgements}
The Flatiron Institute is a division of the Simons Foundation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                  The Bibliography                       %%
%%                                                         %%
%%  Bmc_mathpys.bst  will be used to                       %%
%%  create a .BBL file for submission.                     %%
%%  After submission of the .TEX file,                     %%
%%  you will be prompted to submit your .BBL file.         %%
%%                                                         %%
%%                                                         %%
%%  Note that the displayed Bibliography will not          %%
%%  necessarily be rendered by Latex exactly as specified  %%
%%  in the online Instructions for Authors.                %%
%%                                                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{bmc-mathphys}
\bibliography{bib}

%% Appendices

\section*{Dataset}
\label{sec:dataset}

The dataset considered in this paper as been recorded at Ircam and is composed of audio recording of $16$ different musical instruments played with different PTs, which leads to $235$ different IPT pairs.
In addition, the pitch and nuance is varie for each pair (when applicable), yielding a total of $25444$ audio recordings.
For each couple, the pitch and intonation is varied leading to 25444 audio samples.

The recorded instruments belongs to the wind and string classes.
For winds, the instruments considered in the second experiment are: accordion (Acc), saxophone alto (ASax), tuba (Tb), bassoon (Bn), clarinet Bb (BbCl), flute (Fl), horn (Hn), oboe (Ob), trombone tenor (TTbn), and trumpet C (TpC).
For the string class, the instruments are: guitar (Gtr), harp (Hp), viola (Va), violin (Vn), violoncello (Vc), and contrabass (Cb).
\ja{Better to display these in a table?}

Some instruments can be complemented with sordina (S), wha (W).
The addition of this kind of device modifies the shape of the instrument.
Consequently, an alto saxophone and alto saxophone with sordina are considered as different instruments.
\vl{This is crucial and should not be deferred to the appendix. @writing}
\ml{J'ai mis ca en appendice pour ne pas que le lecteur confonde la base SOL complete et les données considérées dans l'étude.}

Those instruments are recorded for different nuance and pitch if the latter is relevant, but more importantly, several playing technique are considered.
For the second experiment described in this paper, the playing techniques considered are: artificial harmonic (art), blow (blow), exploding slap (explo), harmonic (harm), key-click (key), col legno (legno), non vibrato (nonvib), ordinario (ord), pedal tone (pdl), pizzicato (pizz), ponticello (pont), slap (slap), (tasto), tongue-ram (tng), and xylophonic (xyl).

\vl{This section is not precise enough in terms of intensity ranges and pitch ranges.
The abbreviations can be removed. @data @writing}
\ml{}

\vl{A bubble plot of number of notes per instrument (x axis) and playing technique
(y axis) would be extremely useful. @data @figure}
\ml{Tu voudrais ca pour la base SOL complete ou les données considérées dans l'étude ?}
\vl{Les donn\'{e} de l'\'{e}tude, relatives aux Figures 5 et 6.}

\section*{Human timbre judgments}
\label{sec:subjective}

The dataset considered in this study is taken
from the studio-on-line (SOL) music library  \cite{peeters2000instrument}.
It has audio recordings of 16 different musical instruments played
with different playing techniques,
which leads to $235$ different couples of instrument / playing technique.
For each couple, the pitch and intonation is varied leading to $25444$ recordings.
The way the dataset is structured
is explained in more details in Appendix \ref{sec:dataset}.

Gathering the similarity judgments between each pair of samples would be a daunting task. Indeed, studies about perceptual similarity are hindered with a dimensional problem as the number of pairs to be evaluated grows with the square of the number of items considered.

One reasonable assumption that we choose to consider here is that the pitch and nuance do not alter significantly the perception of timbral similarity. This is in line with the ANSI negative definition of timbre \vl{expliquer, citer, et modérer le propos en donnant des exemples dans lequel le pitch ou la nuance change le timbre}.


\section*{Reducing Expert judgments of technique peculiarities}

\ja{This needs to be summarized (possible in the previous section).
I'm copying two sentences in here that I had in the Timbre similarity judgments section.}%
These IPTs were selected from a larger set of $235$ IPTs by two experts at the \emph{Conservatoire national sup\'erieur de musique et de danse de Paris} (CNSMDP).
The selection criteria were based on the likelihood of an IPT being similar to some other IPT, ensuring that IPTs that were too singular were excluded from the survey.

Even so, considering every pairs of IPTs is not practical. A selection
experiment is thus conducted where the subject is asked to give his opinion on which
\ipts are relevant to study. More precisely, an \ipt is said to be relevant to study
if it is likely to be associated to another \ipt of another instrument.
Interest is rated on a 7 ticks scale. The subject can listen to all the different
nuance and pitch samples of each \ipt. The experiment is over when all the
\ipts are rated.

Rating guidelines are provided :
\begin{itemize}
  \item One star: this \ipt is singular, it is not useful to compare it with another \ipt of another instrument.
  \item Four stars: there is a proximity on one aspect of the sound between this
  \ipt and of an \ipt of another instrument,
  but this one is neither decisive nor obvious.
  \item Seven stars: there is a large similarity between this \ipt and an \ipt of another instrument.
\end{itemize}

Two music composition professors of the CNSMDP performed the test, that is rating each $235$ \ipt.

\vl{This subsection needs a box-and-whisker plot of ratings, where the 235 different
couples of instrument and playing techniques are sorted on the x-axis by average rating. @figure @ratings}

\ml{il n'y que deux raters, ca ne pas pas sens de faire et ca sera illisible}

\vl{Are the histogram of ratings similar for the two raters? It would be good to make a statistical test for this. @research @stats @ratings}

\ml{les histograms sont dans la figure statxp1. Le code pour la generer est dans code/xp1 faire tourner extractData et statData. Un ttest rejette l'hypothese nulle.}

Even if the hypothesis that the distributions of ratings comes from independent random samples from normal distributions with equal means and equal but unknown variances, is verified using the two-sample t-test, inspection of the respective histograms reveals that averaging the ratings of the two raters is reasonable.

Keeping in mind that the gathering of the perceptual judgments of \ipt similarities shall remain practical, the number of \ipts considered shall be lower than $100$. We thus select the \ipts with an average rating higher than three, which brings us 78 \ipts to study.

%%%%%%%%%%%%%% CROWDSOURCED JUDGMENTS OF PAIRWISE SIMILARITY %%%%%%%%%%%%%%%%%%%%
\section*{Crowdsourced judgments of pairwise similarity}

\ja{A lot of this is covered in the main text.
Not sure what we need to add in the appendix.}

The second experiment aims at gathering spontaneous judgment of similarity for those 78 \ipts using a canonical free sorting task. This type of task is chosen among others for two main reasons. First, it gives the ability to the subject to listen to the whole set of \ipts before performing the task. Second, it gives more freedom to the subject, thus potentially more interest to perform the task than a lengthy series of XXY dissimilarity ratings.

The subjects are thus asked to organize the 78 \ipts displayed as grey dots on a two dimensional plane into groups by assigning colors to each dots according to their "acoustic similarity". The textual instructions provided to introduce the experiment was chosen to be deliberately vague in order to allow the subject to follow its own judgment.

The plane is displayed on a computer screen using a web based interface\footnote{The interface is available here: http://soundthings.org/ticel/groupSounds}. The \ipts can be listened to by hovering the mouse on the corresponding dot. The experiment is over when the subject assigned a color to all the dots. The initial location of the dots on the plane is randomized for each subject and can be changed during the experiment by the subject for convenience. Event hough the location of the dots is recorded, this data is not considered during the analysis.

The experiment was available online for two months and requests to perform the test have been sent to internal mailing lists of the CNSMDP and international research mailing with a focus on audio and music processing. The experiment have been performed on a voluntary basis without gratification. Subjects were asked to use headphones at a comfortable listening level.


31 subjects completed the experiment. An example of resulting organization of \ipt is given on Figure \ref{fig:xp2display}. \footnote{The resulting organization for each subject are available for reader's inspection using the full feature interface provided to the subjects for performing the experiment \url{https://mathieulagrange.github.io/paperSpontaneousSimilarity/demo}}.

Only the clustering given by the subjects using the color labels is considered to estimate the spontaneous judgments of similarity among \ipt. The spatial organization of the dots representing the \ipt could provide information about the similarity, but this would implicitly enforce the fact that the timbral similarity space is two dimensional. We therefore now only study the properties of the clusterings performed by the subject prior to converting them into an overall similarity matrix.

This results in $31$ clusterings that can be considered for defining a perceptual timbre space of \ipts. Among the 20 colors available, the subjects used on average $10.2 \pm  4.1$ different colors to group \ipts. The distribution of the number of groups can be seen on Figure \ref{fig:xp2nbGroup}. The size of the groups is on average $7.7 \pm   7.2$. The distribution of the size of groups can be seen on Figure \ref{fig:xp2sizeGroup}. The large number of small groups indicates that many subjects considered that a few of the \ipt were very different from all the remaining \ipt.

\begin{figure}
\center
\includegraphics[width = \textwidth]{figures/nbc.png}
\caption{Histogram of the number of groups.
\vl{The width of this figure should be a half page instead of a full page. Pair with next figure. @figure}}
\ml{ok}
\label{fig:xp2nbGroup}
\end{figure}

\begin{figure}
\center
\includegraphics[width = \textwidth]{figures/sbc.png}
\caption{Histogram of the size of groups.
\vl{The width of this figure should be a half page instead of a full page. Pair with previous figure. @figure}}
\ml{ok}
\label{fig:xp2sizeGroup}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Figures                       %%
%%                               %%
%% NB: this is for captions and  %%
%% Titles. All graphics must be  %%
%% submitted separately and NOT  %%
%% included in the Tex document  %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Figures}

\begin{figure}[h!]
\caption{\csentence{STRF + scattering (upsampling STRF)}
A short description of the figure content should go here.}
\end{figure}

\begin{figure}[h!]
\caption{\csentence{Model overview}
One line music players (3 ipts bass pizz, bass archo, violin archo) sound, spec,
scat, isomap.
Second line : perceptual clustering.}
\end{figure}

\begin{figure}[h!]
\caption{\csentence{Sample scattering}
Same 3 sounds spec + scatt.}
\end{figure}

\begin{figure}[h!]
\caption{\csentence{Tag clouds}
A short description of the figure content should go here.}
\end{figure}

\begin{figure}[h!]
\caption{\csentence{Perfs MFCC, separable, joint (last)}
A short description of the figure content should go here.}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Tables                        %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Tables}
\begin{table}[h!]
\caption{Sample table title. This is where the description of the table should go.}
      \begin{tabular}{cccc}
        \hline
           & B1  &B2   & B3\\ \hline
        A1 & 0.1 & 0.2 & 0.3\\
        A2 & ... & ..  & .\\
        A3 & ..  & .   & .\\ \hline
      \end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Additional Files              %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Additional Files}
  \subsection*{Additional file 1 --- Sample additional file title}
    Additional file descriptions text (including details of how to
    view the file, if it is in a non-standard format or the file extension).  This might
    refer to a multi-page table or a figure.

  \subsection*{Additional file 2 --- Sample additional file title}
    Additional file descriptions text.

\end{backmatter}

\end{document}
