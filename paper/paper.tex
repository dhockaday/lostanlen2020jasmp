% Template for PLoS
% Version 3.5 March 2018
%
% % % % % % % % % % % % % % % % % % % % % %
%
% -- IMPORTANT NOTE
%
% This template contains comments intended 
% to minimize problems and delays during our production 
% process. Please follow the template instructions
% whenever possible.
%
% % % % % % % % % % % % % % % % % % % % % % % 
%
% Once your paper is accepted for publication, 
% PLEASE REMOVE ALL TRACKED CHANGES in this file 
% and leave only the final text of your manuscript. 
% PLOS recommends the use of latexdiff to track changes during review, as this will help to maintain a clean tex file.
% Visit https://www.ctan.org/pkg/latexdiff?lang=en for info or contact us at latex@plos.org.
%
%
% There are no restrictions on package use within the LaTeX files except that 
% no packages listed in the template may be deleted.
%
% Please do not include colors or graphics in the text.
%
% The manuscript LaTeX source should be contained within a single file (do not use \input, \externaldocument, or similar commands).
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% -- FIGURES AND TABLES
%
% Please include tables/figure captions directly after the paragraph where they are first cited in the text.
%
% DO NOT INCLUDE GRAPHICS IN YOUR MANUSCRIPT
% - Figures should be uploaded separately from your manuscript file. 
% - Figures generated using LaTeX should be extracted and removed from the PDF before submission. 
% - Figures containing multiple panels/subfigures must be combined into one image file before submission.
% For figure citations, please use "Fig" instead of "Figure".
% See http://journals.plos.org/plosone/s/figures for PLOS figure guidelines.
%
% Tables should be cell-based and may not contain:
% - spacing/line breaks within cells to alter layout or alignment
% - do not nest tabular environments (no tabular environments within tabular environments)
% - no graphics or colored text (cell background color/shading OK)
% See http://journals.plos.org/plosone/s/tables for table guidelines.
%
% For tables that exceed the width of the text column, use the adjustwidth environment as illustrated in the example table in text below.
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% -- EQUATIONS, MATH SYMBOLS, SUBSCRIPTS, AND SUPERSCRIPTS
%
% IMPORTANT
% Below are a few tips to help format your equations and other special characters according to our specifications. For more tips to help reduce the possibility of formatting errors during conversion, please see our LaTeX guidelines at http://journals.plos.org/plosone/s/latex
%
% For inline equations, please be sure to include all portions of an equation in the math environment.  For example, x$^2$ is incorrect; this should be formatted as $x^2$ (or $\mathrm{x}^2$ if the romanized font is desired).
%
% Do not include text that is not math in the math environment. For example, CO2 should be written as CO\textsubscript{2} instead of CO$_2$.
%
% Please add line breaks to long display equations when possible in order to fit size of the column. 
%
% For inline equations, please do not include punctuation (commas, etc) within the math environment unless this is part of the equation.
%
% When adding superscript or subscripts outside of brackets/braces, please group using {}.  For example, change "[U(D,E,\gamma)]^2" to "{[U(D,E,\gamma)]}^2". 
%
% Do not use \cal for caligraphic font.  Instead, use \mathcal{}
%
% % % % % % % % % % % % % % % % % % % % % % % % 
%
% Please contact latex@plos.org with any questions.
%
% % % % % % % % % % % % % % % % % % % % % % % %

\documentclass[10pt,letterpaper]{article}
\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}

% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% Use Unicode characters when possible
\usepackage[utf8x]{inputenc}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% ligatures disabled
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

% array package and thick rules for tables
\usepackage{array}

% our packages
\usepackage{url}
\usepackage{color}
\usepackage{xspace}

% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% create \thickcline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
  \noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
  \cline{#1}%
  \noalign{\vskip\arrayrulewidth}%
  \noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}


% Remove comment for double spacing
%\usepackage{setspace} 
%\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in 
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
\bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother



% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
%\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
%\setlength{\headheight}{27.023pt}
%\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\today}

%% Include all macros below

\newcommand{\lorem}{{\bf LOREM}}
\newcommand{\ipsum}{{\bf IPSUM}}

%% END MACROS SECTION

%% BEGIN OUR MACROS SECTION

\newcommand*{\eg}{e.g.\@\xspace}
\newcommand*{\ie}{i.e.\@\xspace}
\newcommand{\ipt}{IPT\xspace}
\newcommand{\ipts}{IPTs\xspace}

\newcommand{\patk}{p\mathrm{@}k}

\newcommand{\ml}[1]{\textcolor{blue}{ML: #1}}
\newcommand{\vl}[1]{\textcolor{red}{VL: #1}}
\newcommand{\ja}[1]{\textcolor{purple}{JA: #1}}

%% END OUR MACROS SECTION

\begin{document}
\vspace*{0.2in}

% Title must be 250 characters or less.
\begin{flushleft}
{\Large
\textbf\newline{Learning Auditory Similarities Between Instrumental Playing Techniques} % Please use "sentence case" for title and headings (capitalize only the first word in a title (or heading), the first word in a subtitle (or subheading), and any proper nouns).
}
\newline
% Insert author names, affiliations and corresponding author email (do not include titles, positions, or degrees).
\\
Christian El-Hajj\textsuperscript{1},
Vincent Lostanlen\textsuperscript{2},
Mathias Rossignol\textsuperscript{3},
Gr\'egoire Lafay\textsuperscript{1},
Mathieu Lagrange\textsuperscript{2*}
\\
\bigskip
\textbf{1} Affiliation Dept/Program/Center, Institution Name, City, State, Country
\\
\textbf{2} Affiliation Dept/Program/Center, Institution Name, City, State, Country
\\
\textbf{3} Affiliation Dept/Program/Center, Institution Name, City, State, Country
\\
\bigskip

% Use the asterisk to denote corresponding authorship and provide email address in note below.
* correspondingauthor@institute.edu

\end{flushleft}
% Please keep the abstract below 300 words
\section*{Abstract}
ABSTRACT HERE

% Please keep the Author Summary between 150 and 200 words
% Use first person. PLOS ONE authors please skip this step. 
% Author Summary not valid for PLOS ONE submissions.   
\section*{Author summary}
AUTHOR SUMMARY HERE

\linenumbers

\section*{Figures}

Fig 1: strf + scattering (upsampling strf)

Fig 2: one liner music players (3 ipts bass pizz, bass archo, violin archo) sound, spec, scat, isomap
other line : perceptual clustering

Fig 3: same 3 sounds spec + scatt

Fig 4: tags clouds

Fig last: perfs mfcc separable joint


% Use "Eq" instead of "Equation" for equation citations.
\section*{Introduction}
\label{sec:introduction}

% Section outline:
% - How to accurately model timbre perception?
% - Studies either consider stimuli that are ``too simple'' (e.g., tones, instruments with fixed playing techniques) or ``too complex'' (e.g., speech, environmental sounds).
% - Computational models do not capture enough information (MFCCs and related representations) or are hard to compute or analyze (STRFs, deep networks).
% - Present a study of timbral similarity perception on diverse set of instrumental sounds with extended playing techniques (instrumental playing techniques, or IPTs). Provides more control in modeling different aspects of timbre.
% - In addition, propose a computational model for timbral similarity: (joint) scattering plus a linear layer. Fixed representation to extract time-frequency structure with adaptivity in linear component.

One of the most important properties for identifying the source of a sound is its \emph{timbre}.
While it is typically defined as the quality of a sound that is independent of its pitch, duration, or loudness, this definition fails to provide an explicit characterization of the attribute.
Due to its central role in audio perception, timbre has been studied from several angles, including perceptual and psychophysical studies as well as computational methods in machine listening.
The consensus is that timbre depends not on the purely spectral or temporal structure of a sound, but in its joint spectro--temporal structure, but beyond this, there is little agreement about its precise characterization.

One difficulty that arises in studying the perceptual dimensions of timbre relates to its potential complexity---the dimensionality of the full timbral space is high.
To combat this, many perceptual and psychophysical studies focus on simple stimuli: pure sine waves, musical instruments with fixed pitch and playing technique, and so on.
While this has led to important insights in timbre perception, the range of timbres explored by these stimuli is necessarily limited.

On the other end of the spectrum are the computational methods.
Since their goals are typically task-oriented (recognizing speech, identifying environmental sounds, and so on), they are less concerned with establishing models of auditory perception suitable for analysis.
While certain audio representations are simple enough to analyze, such as mel-frequency cepstral coefficients (MFCCs) and related approaches, they do not capture enough structure to adequately characterize timbre.
Indeed, for an overwhelming majority of machine listening tasks have been these representations have been outperformed by learned representations, such as deep neural networks (DNNs).
While DNNs have enjoyed significant success for these tasks, their relative complexity and the fact that they are learned from data make their analysis difficult, preventing us from deriving relevant insights into timbre perception.

Certain audio representations have been proposed as a middle ground between expressivity and ease of analysis.
One of the more prominent among these are the \emph{cortical representations} introduced by S. Shamma and collaborators.
These rely on a spectrogram-like decomposition of the sound, which is then further decomposed in time and frequency using \emph{spectro--temporal receptive fields} (STRFs).
This model is based on neurophysiological measurements on ferrets, and have been validated in several contexts, ranging from applications in machine listening and reproduction of psychophysical phenomena in auditory perception.
However, since the original goal of the representation was to model observed electrophysiological signals, this increases its complexity, rendering mathematical anaylsis more difficult.

In particular, cortical representations have been used to study timbre in the work of Patil et al. \cite{patil2012music}.
While this demonstrates the power of the representation to characterize certain dimensions of timbre, the range of sounds was limited, consisting of single musical notes played using standard instrumental techniques.
As a result, only a subset of the relevant timbral dimensions are explored, limiting the conclusions of the work.

In this work, we consider representation similar to the cortical representation, namely the \emph{scattering transform}, which provides a mathematical foundation for the study of auditory perception in terms of invariance and stability properties.
Instead of single musical notes played with standard techniques, we extend the range of sounds to include extended instrumental playing techniques.
While increasing the complexity of the timbral space explored, these sounds do not include the full range of timbre exhibited, for example, by speech or environmental sounds.
As a result, we are able to better analyze their characteristics and link them to the properties of the transform.

\section*{Results}

% Section outline:
% - Subjects asked to cluster 78 IPTs into an arbitrary number of clusters (maximum 20). Resulting partition gives measure of timbral similarity.
% - Partitions do not completely agree with either instrument (I) of playing technique (PT) taxonomies, although slightly favor PTs. Perceptual measurement captures timbre across IPTs. Show through NMI or other measurements.
% - Propose computational model for these perceptual similarity measurements: scattering transform (time or time-frequency), followed by a learned linear layer (LDA or LDMM) to adapt representation to measured data.
% - Model shows good agreement with similarity measurements under different testing protocols (aggregated clusters, aggregated subspaces, cross validation).

\subsection*{Timbre similarity perception}
\label{sec:survey}

To validate our timbre perception model, we first acquired similarity judgments from a survey of human subjects.
The survey was carried out for $78$ sounds, each representing an instrument--playing technique (IPT) pair.
These IPTs were selected from a larger set of $235$ IPTs by two experts at the \emph{Conservatoire national sup\'erieur de musique et de danse de Paris} (CNSMDP).
The selection criteria were based on the likelihood of an IPT being similar to some other IPT, ensuring that IPTs that were too singular were excluded from the survey.

In the survey, a subject was provided with these $78$ IPTs at the same pitch and asked to cluster them in a free sorting task.
This was achieved by arranging correspoding dots in a two-dimensional plane on a computer screen and assigning a color to each dot.
While the location of the dots was recorded, this information was not used in our analysis and only served to help the subject organize the sounds spatially.
The colors assigned to each dot---and, consequently, to each sound---was recorded for each subject.

The above similarity judgments were crowdsourced by sending out requests to perform the experiment on internal mailing lists of the CNSMDP and international research mailing lists with focus on audio and music processing.
We made the experiment available for two months and obtained results for a total of $31$ subjects.

Given the clustering assignments of each subject, we then constructed a consensus clustering using a hypergraph partitioning algorithm \cite{kernighan1970efficient,han1997scalable,strehl2002cluster}.
Interestingly, although several sounds pairs were obtained from the same instrument (but different PTs) and others were obtained using the same PT (but different instruments), the consensus clustering did not favor one of these taxonomies over the other.

\subsection*{Scattering transform}
\label{sec:scattering}

While cortical representations have been successful in distinguishing properties of audio recordings (including similarity analysis and classification tasks), they remain models for the electrophysiological signals recorded in the audio cortex.
As such, their calculation is not always computationally efficient and their mathematical properties may be difficult to analyze directly.

A closely related representation is that of the time scattering transform, introduced by And\'{e}n and Mallat \cite{anden2011multiscale,anden2014deep}.
Given an audio recording, it decomposes the signal using an analytic wavelet transform and computes the modulus, obtaining the \emph{scalogram} of the signal.
The scalogram is similar to the spectrogram, but has a time--frequency resolution that varies with frequency: at low frequency, the filters are narrowly tuned in frequency (and hence wide in time), while at high frequency, the filters are wide in frequency (and hence narrowly concentrated in time).
This representation contains a large amount of information, but not in a form suitable for building statistical models.
In particular, the scalogram is not \emph{stable} in the sense that small deformations to the original signal may induce large changes to the scalogram.
It also lacks \emph{invariance} to time-shifting, which does not affect our perception of a sound, and should therefore not affect its timbre model.
One way to resolve this is to average the scalogram in time to obtain a spectral profile of the signal, yielding the first-order scattering coefficients.
These possess both invariance to time-shifting and stability to time-warping deformations \cite{anden2014deep}.

Since these first-order coefficients only include spectral information, they cannot accurately characterize more sophisticated structure in the signal.
One way to augment them is to calculate another wavelet decomposition in time, this time on each frequency channel of the scalogram.
Again, we compute the modulus of the wavelet coefficients and average in time.
The resulting coefficients are known as the second-order time scattering coefficients.
These also possess the desired invariance and stability properties, but capture more information on the temporal structure of the scalogram \cite{anden2014deep}.

While richer than a simple spectral decomposition, such as MFCCs, the second-order time scattering coefficients described above primarily capture structure in time.
Indeed, they do not explicitly encode frequency structure in an invariant and stable manner.
If a signal is transposed or warped in frequency by a small amount, we do not expect its timbre to change significantly.
We would therefore like our representation to have the same invariance and stability properties along the frequency axis.
This is achieved by taking first- and second-order scattering coefficients and computing a second scattering transform, this time along the log-frequency axis.
The resulting coefficients are known as separable time and frequency scattering coefficients \cite{anden2014deep}.

This representation does possess the desired invariance and stability properties, but its separable nature limits its power to adequately represent time--frequency structures that do not separate cleanly in time and frequency, such as chirps.
To remedy this, another approach was suggested by And\'{e}n et al. \cite{anden2015joint,anden2019joint} wherein the temporal wavelet decomposition of the scalogram is replaced with a two-dimensional, spectro-temporal decomposition.
Similarly to the cortical representation, the result is a four-dimensional array indexed by time, acoustic frequency, modulation frequency (or \emph{rate}), and spectral modulation frequency, known as \emph{quefrency} (or \emph{scale}).
As before, we compute the modulus and average in time, obtaining a set of coefficients known as second-order joint time--frequency scattering coefficients.

The definition of the scattering transform in terms of wavelet decompositions and modulus nonlinearities presents several advantages.
On the computational side, the critical bandwidth guarantees of wavelet transforms allows us to judiciously subsample the output, resulting in a lower-dimensional representation compared to the cortical representations.
The scattering transform also satisfies the aforementioned stability conditions, providing a guarantee that slightly deforming a signal only results in a negligible change in its scattering transform.
Furthermore, we may calculate the scattering tranforms of several model signals, including amplitude-modulated harmonic sounds and noise excitations \cite{anden2012scattering,anden2014deep}, frequency-modulated sinusoids (e.g., chirps) \cite{anden2012scattering,anden2019joint}, and beating phenomena \cite{anden2014deep}.
They have also enjoyed success in several audio classification and similarity retrieval tasks \cite{anden2011multiscale,bauge2013representing,anden2014deep,anden2019joint,lostanlen2018relevance,lostanlen2018extended}.

To evaluate the ability of the scattering transform to characterize the timbre of an audio signal, we consider the task of classifying musical notes from a wide variety of instruments played according to a set of extended instrumental playing techniques (IPTs).
Increasing the variability in IPTs allows us to explore a wider range of timbre, providing a more robust validation of the proposed model for modeling timbre.
The metric used is the \emph{precision at rank $k$} ($\patk$), which is calculated by computing the $k$ nearest neighbors in the feature space and recording the proportion of the neighbors that are in the same class.
In the following, we set $k = 5$.
The classes were obtained from the consensus clustering of human similarity judgments described in the previous section.
The $\patk$ therefore provides a measure of consistency between the timbre similarity measures obtained by our model and those of the survey subjects.

Concatenating the first- and second-order separable time and frequency scattering coefficients, we obtain a feature vector of dimension $???$ for a sound recording of duration $???$.
Computing nearest neighbors with the standard Euclidean measure gives a $\patk$ of $0.870 \pm 0.058$, where the first value indicates the mean and the second the standard deviation.
As a result, this simple transformation of the original signal results in a representation that lends itself naturally to discriminating timbre.
Note that this result is obtained by simply looking at the raw feature vectors---no learning is performed to adapt the transform towards discriminating timbre.
Replacing the separable second-order coefficients with the joint second-order coefficients boosts the $\patk$ to $???$.
This is evidence that accurately characterizing the joint time--frequency structure of the sound is important capturing its timbre.

We performed the same evaluation with a standard MFCC representation, obtaining a $\patk$ of $0.851 \pm 0.062$.
As a result, we conclude that a Euclidean distance over scattering transforms (separable or joint) yield little additional perceptual information compared to standard spectral descriptors.

\subsection*{Similarity weighting}
\label{sec:weighting}

To improve our measure, we need to replace the standard Euclidean distance on the raw scattering coefficients by something better adapted to discriminating timbre.
Equivalently, we can apply further transformations to the scattering coefficients and compute the Euclidean distance on the result.
Very flexible and powerful methods can be brought to bear on this problem, such as deep neural networks, but with their flexibility comes an increase in complexity.
This increased complexity of the model reduces its usefulness with respect to interpretability.

For this reason, we restrict ourselves to a simple linear transform of the scattering coefficients.
Specifically, we construct a positive semidefinite matrix $L$ that we apply to the scattering coefficients $x$ so that the Euclidean distance between $Lx$ and $Ly$ best approximates the consensus clustering obtained above.
We expect the resulting representation $Lx$ to provide a more accurate model for timbre perception.

We now use the consensus clustering to reweight the similarity between feature vectors (either MFCCs or scattering transforms).
For this, we use the \emph{large margin nearest neighbor} (LMNN) metric learning algorithm \cite{weinberger2006distance, weinberger2009distance}.
This approach constructs a positive semi-definite weighting matrix $L$ such that the distance $\|Lx - Ly\|$ best classifies a set of points into a fixed clustering assignment.
Running LMNN on MFCCs with the consensus clustering obtained above increases $\patk$ slightly to $0.862 \pm 0.059$.
Consequently, there seems to be little additional timbre information in the MFCCs compared to that given by the standard Euclidean distance.
We contrast this with LMNN applied to the separable scattering coefficients, which yields a $\patk$ of $0.948 \pm 0.033$, which is a significant increase.
For the joint scattering coefficients, we obtain a $\patk$ of $???$.
We therefore conclude that a simple reweighting of the scattering transform yields a more appropriate timbral similarity measure.

\section*{Discussion}
\label{sec:discussion}

\begin{itemize}
\item Demonstrates complex timbral structure perceived by subjects in study: neither instrument (i.e., spectral envelope) or playing technique (i.e., temporal modulation) dominates similarity judgment. Need joint to characterize time-frequency structure (see Patil et al).
\item Proposed computational model (scattering + linear) reproduces similarity judgments accurately. Easy to ``retrain'' for other data. Fixed wavelet structure allows for analysis of linear layer and guarantees invariance and stability properties.
\item Applications: query-by-example in musical sample databases.
\end{itemize}

\section*{Materials and methods}
\label{sec:methods}

\begin{itemize}
\item IPTs pre-screened by two composition teachers to select 78 ``interesting'' IPTs that are very similar to (i.e., easily confused with) other IPTs.
\item Subjects asked to perform free sorting task on the selected IPTs according to similarity. Allowed to label with up to 20 different colors.
\item Clustering gives similarity measurement for each subject. These measurements were aggregated in a certain way.
\item The aggregated clustering was compared to instrument and playing technique taxonomies using NMI or other measures.
\item The (time or time-frequency) scattering transform is composed of alternating layers of wavelet transforms and modulus nonlinearities. The result is a convolutional network with fixed filters. Performs well in various classification and regression tasks when augmented with a learned linear layer.
\item Linear layer is provided here by LDA or LMNN.
\item Performance is measured in a query-by-example setting with precision at $k$, where $k = 5$.
\end{itemize}

\section*{Supporting information}
\label{sec:supp}

\section*{Acknowledgments}
\label{sec:ack}

\nolinenumbers

\bibliographystyle{plos2015}
\bibliography{bib}

\end{document}

