%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% HEADER %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{article}
\usepackage{amsmath,cite,url}
\usepackage{graphicx}
\usepackage{color}
\usepackage{xspace}

\newcommand*{\eg}{e.g.\@\xspace}
\newcommand*{\ie}{i.e.\@\xspace}
\newcommand{\ipt}{IPT\xspace}
\newcommand{\ipts}{IPTs\xspace}

\newcommand{\ml}[1]{\textcolor{blue}{ML: #1}}
\newcommand{\vl}[1]{\textcolor{red}{VL: #1}}
\newcommand{\ja}[1]{\textcolor{purple}{JA: #1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TITLE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Learning Auditory Similarities Between Instrumental Playing Techniques}

\author{
Christian El-Hajj,
Vincent Lostanlen,
Mathias Rossignol, \\
Gr\'egoire Lafay,
and Mathieu Lagrange}

\begin{document}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ABSTRACT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
ABSTRACT HERE
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% INTRODUCTION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:introduction}

\textbf{Section outline:}
\begin{itemize}
\item How to accurately model timbre perception?
\item Studies either consider stimuli that are ``too simple'' (e.g., tones, instruments with fixed playing techniques) or ``too complex'' (e.g., speech, environmental sounds).
\item Computational models do not capture enough information (MFCCs and related representations) or are hard to compute or analyze (STRFs, deep networks).
\item Present a study of timbral similarity perception on diverse set of instrumental sounds with extended playing techniques (instrumental playing techniques, or IPTs). Provides more control in modeling different aspects of timbre.
\item In addition, propose a computational model for timbral similarity: (joint) scattering plus a linear layer. Fixed representation to extract time-frequency structure with adaptivity in linear component.
\end{itemize}

One of the most important properties for identifying the source of a sound is its \emph{timbre}.
While it is typically defined as the quality of a sound that is independent of its pitch, duration, or loudness, this definition fails to provide an explicit characterization of the attribute.
Due to its central role in audio perception, timbre has been studied from several angles, including perceptual and psychophysical studies as well as computational methods in machine listening.
The consensus is that timbre depends not on the purely spectral or temporal structure of a sound, but in its joint spectro--temporal structure, but beyond this, there is little agreement about its precise characterization.

One difficulty that arises in studying the perceptual dimensions of timbre relates to its potential complexity---the dimensionality of the full timbral space is high.
To combat this, many perceptual and psychophysical studies focus on simple stimuli: pure sine waves, musical instruments with fixed pitch and playing technique, and so on.
While this has led to important insights in timbre perception, the range of timbres explored by these stimuli is necessarily limited.

On the other end of the spectrum are the computational methods.
Since their goals are typically task-oriented (recognizing speech, identifying environmental sounds, and so on), they are less concerned with establishing models of auditory perception suitable for analysis.
While certain audio representations are simple enough to analyze, such as mel-frequency cepstral coefficients (MFCCs) and related approaches, they do not capture enough structure to adequately characterize timbre.
Indeed, for an overwhelming majority of machine listening tasks have been these representations have been outperformed by learned representations, such as deep neural networks (DNNs).
While DNNs have enjoyed significant success for these tasks, their relative complexity and the fact that they are learned from data make their analysis difficult, preventing us from deriving relevant insights into timbre perception.

Certain audio representations have been proposed as a middle ground between expressivity and ease of analysis.
One of the more prominent among these are the \emph{cortical representations} introduced by S. Shamma and collaborators.
These rely on a spectrogram-like decomposition of the sound, which is then further decomposed in time and frequency using \emph{spectro--temporal receptive fields} (STRFs).
This model is based on neurophysiological measurements on ferrets, and have been validated in several contexts, ranging from applications in machine listening and reproduction of psychophysical phenomena in auditory perception.
However, since the original goal of the representation was to model observed electrophysiological signals, this increases its complexity, rendering mathematical anaylsis more difficult.

In particular, cortical representations have been used to study timbre in the work of Patil et al. \cite{patil2012music}.
While this demonstrates the power of the representation to characterize certain dimensions of timbre, the range of sounds was limited, consisting of single musical notes played using standard instrumental techniques.
As a result, only a subset of the relevant timbral dimensions are explored, limiting the conclusions of the work.

In this work, we consider representation similar to the cortical representation, namely the \emph{scattering transform}, which provides a mathematical foundation for the study of auditory perception in terms of invariance and stability properties.
Instead of single musical notes played with standard techniques, we extend the range of sounds to include extended instrumental playing techniques.
While increasing the complexity of the timbral space explored, these sounds do not include the full range of timbre exhibited, for example, by speech or environmental sounds.
As a result, we are able to better analyze their characteristics and link them to the properties of the transform.


\section{Results}
\label{sec:results}

\begin{itemize}
\item Subjects asked to cluster 78 IPTs into an arbitrary number of clusters (maximum 20). Resulting partition gives measure of timbral similarity.
\item Partitions do not completely agree with either instrument (I) of playing technique (PT) taxonomies, although slightly favor PTs. Perceptual measurement captures timbre across IPTs. Show through NMI or other measurements.
\item Propose computational model for these perceptual similarity measurements: scattering transform (time or time-frequency), followed by a learned linear layer (LDA or LDMM) to adapt representation to measured data.
\item Model shows good agreement with similarity measurements under different testing protocols (aggregated clusters, aggregated subspaces, cross validation).
\end{itemize}

\section{Discussion}
\label{sec:discussion}

\begin{itemize}
\item Demonstrates complex timbral structure perceived by subjects in study: neither instrument (i.e., spectral envelope) or playing technique (i.e., temporal modulation) dominates similarity judgment. Need joint to characterize time-frequency structure (see Patil et al).
\item Proposed computational model (scattering + linear) reproduces similarity judgments accurately. Easy to ``retrain'' for other data. Fixed wavelet structure allows for analysis of linear layer and guarantees invariance and stability properties.
\item Applications: query-by-example in musical sample databases.
\end{itemize}

\section{Methods}
\label{sec:methods}

\begin{itemize}
\item IPTs pre-screened by two composition teachers to select 78 ``interesting'' IPTs that are very similar to (i.e., easily confused with) other IPTs.
\item Subjects asked to perform free sorting task on the selected IPTs according to similarity. Allowed to label with up to 20 different colors.
\item Clustering gives similarity measurement for each subject. These measurements were aggregated in a certain way.
\item The aggregated clustering was compared to instrument and playing technique taxonomies using NMI or other measures.
\item The (time or time-frequency) scattering transform is composed of alternating layers of wavelet transforms and modulus nonlinearities. The result is a convolutional network with fixed filters. Performs well in various classification and regression tasks when augmented with a learned linear layer.
\item Linear layer is provided here by LDA or LMNN.
\item Performance is measured in a query-by-example setting with precision at $k$, where $k = 5$.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% BIBLIOGRAPHY %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{alpha}
\bibliography{bib}

\end{document}
