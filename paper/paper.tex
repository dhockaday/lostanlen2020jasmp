%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% HEADER %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{article}
\usepackage{amsmath,cite,url}
\usepackage{graphicx}
\usepackage{color}
\usepackage{xspace}

\newcommand*{\eg}{e.g.\@\xspace}
\newcommand*{\ie}{i.e.\@\xspace}
\newcommand{\ipt}{IPT\xspace}
\newcommand{\ipts}{IPTs\xspace}

\newcommand{\patk}{p\mathrm{@}k}

\newcommand{\ml}[1]{\textcolor{blue}{ML: #1}}
\newcommand{\vl}[1]{\textcolor{red}{VL: #1}}
\newcommand{\ja}[1]{\textcolor{purple}{JA: #1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TITLE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Learning Auditory Similarities Between Instrumental Playing Techniques}

\author{
Christian El-Hajj,
Vincent Lostanlen,
Mathias Rossignol, \\
Gr\'egoire Lafay,
and Mathieu Lagrange}

\begin{document}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ABSTRACT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
ABSTRACT HERE
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% INTRODUCTION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:introduction}

\textbf{Section outline:}
\begin{itemize}
\item How to accurately model timbre perception?
\item Studies either consider stimuli that are ``too simple'' (e.g., tones, instruments with fixed playing techniques) or ``too complex'' (e.g., speech, environmental sounds).
\item Computational models do not capture enough information (MFCCs and related representations) or are hard to compute or analyze (STRFs, deep networks).
\item Present a study of timbral similarity perception on diverse set of instrumental sounds with extended playing techniques (instrumental playing techniques, or IPTs). Provides more control in modeling different aspects of timbre.
\item In addition, propose a computational model for timbral similarity: (joint) scattering plus a linear layer. Fixed representation to extract time-frequency structure with adaptivity in linear component.
\end{itemize}

One of the most important properties for identifying the source of a sound is its \emph{timbre}.
While it is typically defined as the quality of a sound that is independent of its pitch, duration, or loudness, this definition fails to provide an explicit characterization of the attribute.
Due to its central role in audio perception, timbre has been studied from several angles, including perceptual and psychophysical studies as well as computational methods in machine listening.
The consensus is that timbre depends not on the purely spectral or temporal structure of a sound, but in its joint spectro--temporal structure, but beyond this, there is little agreement about its precise characterization.

One difficulty that arises in studying the perceptual dimensions of timbre relates to its potential complexity---the dimensionality of the full timbral space is high.
To combat this, many perceptual and psychophysical studies focus on simple stimuli: pure sine waves, musical instruments with fixed pitch and playing technique, and so on.
While this has led to important insights in timbre perception, the range of timbres explored by these stimuli is necessarily limited.

On the other end of the spectrum are the computational methods.
Since their goals are typically task-oriented (recognizing speech, identifying environmental sounds, and so on), they are less concerned with establishing models of auditory perception suitable for analysis.
While certain audio representations are simple enough to analyze, such as mel-frequency cepstral coefficients (MFCCs) and related approaches, they do not capture enough structure to adequately characterize timbre.
Indeed, for an overwhelming majority of machine listening tasks have been these representations have been outperformed by learned representations, such as deep neural networks (DNNs).
While DNNs have enjoyed significant success for these tasks, their relative complexity and the fact that they are learned from data make their analysis difficult, preventing us from deriving relevant insights into timbre perception.

Certain audio representations have been proposed as a middle ground between expressivity and ease of analysis.
One of the more prominent among these are the \emph{cortical representations} introduced by S. Shamma and collaborators.
These rely on a spectrogram-like decomposition of the sound, which is then further decomposed in time and frequency using \emph{spectro--temporal receptive fields} (STRFs).
This model is based on neurophysiological measurements on ferrets, and have been validated in several contexts, ranging from applications in machine listening and reproduction of psychophysical phenomena in auditory perception.
However, since the original goal of the representation was to model observed electrophysiological signals, this increases its complexity, rendering mathematical anaylsis more difficult.

In particular, cortical representations have been used to study timbre in the work of Patil et al. \cite{patil2012music}.
While this demonstrates the power of the representation to characterize certain dimensions of timbre, the range of sounds was limited, consisting of single musical notes played using standard instrumental techniques.
As a result, only a subset of the relevant timbral dimensions are explored, limiting the conclusions of the work.

In this work, we consider representation similar to the cortical representation, namely the \emph{scattering transform}, which provides a mathematical foundation for the study of auditory perception in terms of invariance and stability properties.
Instead of single musical notes played with standard techniques, we extend the range of sounds to include extended instrumental playing techniques.
While increasing the complexity of the timbral space explored, these sounds do not include the full range of timbre exhibited, for example, by speech or environmental sounds.
As a result, we are able to better analyze their characteristics and link them to the properties of the transform.


\section{Results}
\label{sec:results}

\textbf{Section outline:}
\begin{itemize}
\item Subjects asked to cluster 78 IPTs into an arbitrary number of clusters (maximum 20). Resulting partition gives measure of timbral similarity.
\item Partitions do not completely agree with either instrument (I) of playing technique (PT) taxonomies, although slightly favor PTs. Perceptual measurement captures timbre across IPTs. Show through NMI or other measurements.
\item Propose computational model for these perceptual similarity measurements: scattering transform (time or time-frequency), followed by a learned linear layer (LDA or LDMM) to adapt representation to measured data.
\item Model shows good agreement with similarity measurements under different testing protocols (aggregated clusters, aggregated subspaces, cross validation).
\end{itemize}

\subsection{Scattering transform}
\label{sec:scattering}

While cortical representations have been successful in distinguishing properties of audio recordings (including similarity analysis and classification tasks), they remain models for the electrophysiological signals recorded in the audio cortex.
As such, their calculation is not always computationally efficient and their mathematical properties may be difficult to analyze directly.

A closely related representation is that of the joint time--frequency scattering transform, introduced by And\'{e}n et al. \cite{anden2015joint,anden2019joint}.
Given an audio recording, it decomposes the signal using an analytic wavelet transform and computes the modulus, obtaining the \emph{scalogram} of the signal.
The scalogram is similar to the spectrogram, but has a time--frequency resolution that varies with frequency: at low frequency, the filters are narrowly tuned in frequency (and hence wide in time), while at high frequency, the filters are wide in frequency (and hence narrowly concentrated in time).
This representation contains a large amount of information, but not in a form suitable for building statistical models.
In particular, the scalogram is not \emph{stable} in the sense that small deformations to the original signal may incude large changes to the scalogram.
One way to resolve this is to average the scalogram in time to obtain a spectral profile of the signal, yielding the first-order scattering coefficients \cite{anden2014deep}.

These first-order coefficients alone cannot accurately characterize the time--frequency structure of the signal, however, since they only include spectral information.
To augment them, we perform another wavelet decomposition, this time in two dimensions, over the scalogram.
Similarly to the cortical representation, the result is a four-dimensional array indexed by time, acoustic frequency, modulation frequency (or \emph{rate}), and spectral modulation frequency, known as \emph{quefrency} (or \emph{scale}).
Again, we compute the modulus of the transform and average in time.
The result is known as the set of second-order joint time--frequency scattering coefficients \cite{anden2015joint,anden2019joint}.

The definition of the scattering transform in terms of wavelet decompositions and modulus nonlinearities presents several advantages.
On the computational side, the critical bandwidth guarantees of wavelet transforms allows us to judiciously subsample the output, resulting in a lower-dimensional representation compared to the cortical representations.
The scattering transform also satisfies the aforementioned stability conditions, providing a guarantee that slightly deforming a signal only results in a negligible change in its scattering transform.
Furthermore, we may calculate the scattering tranforms of several model signals, including amplitude-modulated harmonic sounds and noise excitations \cite{anden2012scattering,anden2014deep}, frequency-modulated sinusoids \cite{anden2012scattering,anden2019joint}, and beating phenomena \cite{anden2014deep}.
They have also enjoyed success in several audio classification and similarity retrieval tasks \cite{anden2011multiscale,bauge2013representing,anden2014deep,anden2019joint,lostanlen2018relevance,lostanlen2018extended}.

To evaluate the ability of the scattering transform to characterize the timbre of an audio signal, we consider the task of classifying musical notes from a wide variety of instruments played according to a set of extended instrumental playing techniques (IPTs).
Increasing the variability in IPTs allows us to explore a wider range of timbre, providing a more robust validation of the proposed model for modeling timbre.
The metric used is the \emph{precision at rank $k$} ($\patk$), which is calculated by computing the $k$ nearest neighbors in the feature space and recording the proportion of the neighbors that are in the same ``class.''
In the following, we set $k = 5$.

Concatenating the first- and second-order joint scattering coefficients, we obtain a feature vector of dimension $???$ for a sound recording of duration $???$.
Computing nearest neighbors with the standard Euclidean measure gives a $\patk$ of $87.0 \pm 5.8$, where the first value indicates the mean and the second the standard deviation.
As a result, this simple transformation of the original signal results in a representation that lends itself naturally to discriminating timbre.
Note that this result is obtained by simply looking at the raw feature vectors---no learning is performed to adapt the transform towards discriminating timbre.
We contrast this with the results obtained with a baseline MFCC feature vector.
This results in a $\patk$ of $85.1 \pm 6.2$.
We therefore conclude that, although the scattering transform provides a good indication of timbral similarity, using its nearest neighbors based on a standard Euclidan distance does not allow us to properly account for the higher-level time--frequency structure characterized by the second-order coefficients.

\subsection{Similarity weighting}
\label{sec:weighting}

While the raw scattering representation provides a powerful model for timbre, it does so without any prior information.
Incorporating information about what distinguishes one timbre from another should improve the accuracy of the model.
In the same was that the lower levels of auditory perception do not vary much between subjects (or between species, in the case of mammals), we expect that higher levels incorporate some amount of learning.
Introducing a learned component into our model, however, could greatly reduce its usefulness with respect to interpretability.
For this reason, we restrict ourselves to a simple, linear transform of the scattering coefficients.
By optimizing the linear transform with respect to similarity measurements obtained from human subjects, we expect to obtain a more accurate model of timbre perception.

These human similarity judgments are obtained from a set of $78$ instrument--IPT pairs selected by two music composition professors at the \emph{Conservatoire national sup\'erieur de musique et de danse de Paris} (CNSMDP).
A subject was provided with these 78 recordings and asked to cluster them in a free sorting task by arranging correspodings dots in two dimensions on a computer screen and assigning a color to each dot.
While the location of the dots was recorded, this information was not used in our analysis and only served to help the subject organize the sounds spatially.
The colors assigned to each dot---and, consequently, to each sound---was recorded for each subject.

The above similarity judgments were crowdsourced by sending out requests to perform the experiment on internal mailing lists of the CNSMDP and international research mailing lists with focus on audio and music processing.
We made the experiment available for two months and obtained results for a total of $31$ subjects.

Given the clustering assignments of each subject, we then constructed a consensus clustering using a hypergraph partitioning algorithm \cite{kernighan1970efficient,han1997scalable,strehl2002cluster}.
Interestingly, although several sounds pairs were obtained from the same instrument (but different IPTs) and others were obtained using the same IPT (but different instruments), the consensus clustering did not favor one of these taxonomies over the other.

We now use the consensus clustering to reweight the similarity between feature vectors (either MFCCs or scattering transforms).
For this, we use the \emph{large margin nearest neighbor} (LMNN) metric learning algorithm \cite{weinberger2006distance, weinberger2009distance}.
This approach constructs a positive semi-definite weighting matrix $L$ such that the distance $\|Lx - Ly\|$ best classifies a set of points into a fixed clustering assignment.
Running LMNN on MFCCs with the consensus clustering obtained above increases $\patk$ slightly to $86.2 \pm 5.9$.
Consequently, there seems to be little additional timbre information in the MFCCs compared to that given by the standard Euclidean distance.
We contrast this with LMNN applied to the joint scattering coefficients, which yields a $\patk$ of $94.8 \pm 3.3$, which is a significant increase.
We therefore conclude that a simple reweighting of the scattering transform yields a more appropriate timbral similarity measure.

\section{Discussion}
\label{sec:discussion}

\begin{itemize}
\item Demonstrates complex timbral structure perceived by subjects in study: neither instrument (i.e., spectral envelope) or playing technique (i.e., temporal modulation) dominates similarity judgment. Need joint to characterize time-frequency structure (see Patil et al).
\item Proposed computational model (scattering + linear) reproduces similarity judgments accurately. Easy to ``retrain'' for other data. Fixed wavelet structure allows for analysis of linear layer and guarantees invariance and stability properties.
\item Applications: query-by-example in musical sample databases.
\end{itemize}

\section{Methods}
\label{sec:methods}

\begin{itemize}
\item IPTs pre-screened by two composition teachers to select 78 ``interesting'' IPTs that are very similar to (i.e., easily confused with) other IPTs.
\item Subjects asked to perform free sorting task on the selected IPTs according to similarity. Allowed to label with up to 20 different colors.
\item Clustering gives similarity measurement for each subject. These measurements were aggregated in a certain way.
\item The aggregated clustering was compared to instrument and playing technique taxonomies using NMI or other measures.
\item The (time or time-frequency) scattering transform is composed of alternating layers of wavelet transforms and modulus nonlinearities. The result is a convolutional network with fixed filters. Performs well in various classification and regression tasks when augmented with a learned linear layer.
\item Linear layer is provided here by LDA or LMNN.
\item Performance is measured in a query-by-example setting with precision at $k$, where $k = 5$.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% BIBLIOGRAPHY %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{alpha}
\bibliography{bib}

\end{document}
