The modeling of timbre is typically based on measuring human responses to predefined stimuli, such as musical instruments and speech.
However, the particular stimuli, such as musical notes, are often too simple to explore the complex structure of timbre.
Other stimuli, such as speech sounds, provide a richer timbre, but also carry higher-level meaning, making unbiased responses harder to obtain.
Between these extremes, we consider the use of musical instruments coupled with various instrumental playing techniques (IPTs), offering a broader range of timbre than ordinary notes (i.e., played \textit{ordinario}), while largely avoiding semantic connotations.
We show how human subjects organize these instrument-IPT pairs in free sorting tasks into clusters that suggest a more general taxonomy than that provided by instrument or IPT alone.
In addition, we propose a computational model for reproducing this timbral organization based on joint scattering transforms connected to an unsupervised linear layer.
This model shows good agreement with the timbral similarity judgments of the subjects, providing a promising approach for further exploration of this perceptual dimension.
