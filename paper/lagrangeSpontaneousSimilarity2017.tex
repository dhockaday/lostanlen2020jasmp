
\documentclass{article}
\usepackage{amsmath,cite,url}
\usepackage{graphicx}
\usepackage{color}

\newcommand{\ipt}{IPT}

\newcommand{\ml}[1]{\textcolor{blue}{ML : #1}}

% Title.
% ------
\title{Spontaneous Similarity Judgments of Musical Tones played with various Playing Techniques}

\author{Christian El-Hajj, Mathias Rossignol, Gr√©goire Lafay, Mathieu Lagrange}

\sloppy % please retain sloppy command for improved formatting

\begin{document}
%
\maketitle
%
\begin{abstract}

Musical timbre is a multi-faceted notion that have been extensively studied, mostly focusing on its spectral aspect. By studying the spontaneous judgments of similarity between several instruments played with a diverse set of playing techniques, we aim in this paper at better understanding the influence of the change of the playing technique on the perception of timbre.

A first experiment is conducted by music experts to identify which couple of instrument and playing technique in a given dataset is worth comparing to other couples of instrument / playing technique. In a second experiment, spontaneous similarity judgments among the retained couples are collected using a canonic free sorting task experiment design.


To further study the outcomes of this experiment, we assume a two-step computational model of human perception, where the acoustic signal is fed to a statically designed processing unit that accounts for frequency and temporal modulations. The resulting features are then projected using a supervised technique which considers as input the perceptual similarity judgments gathered in the second experiment. Numerical experiments show that the induced perceptual space is able to approximate perceptual data with satisfying accuracy.

\end{abstract}
%
\section{Introduction}\label{sec:introduction}

Musical timbre is an interesting notion to study as it is perceptively defined. Thus, by studying musical timbre, one study many facets of the human auditory system which responds to a diverse -- but somewhat more controled than other stimuli such as speech or environemntal sounds -- set of physical stimuli.

A popular view of the kind of stimuli we consider is that they can be described by a spread of energy that evolve through time and across frequency. By following this trend, most researchers translate the negation of the ASA definition of timbre into a more relative definition : musical timbre is about the distribution of intensity across the time / frequency plane.

A great deal of litterature focuses on the instaneous spectrum, \textit{i.e.} the timbre can be defined without considering evolution of the energy distribution trough time. Other studies demonstrated the importance of the onset in the recognition of some musical instruments. Those studies on a rather short term model where the waveform is observed in a range that do not exceed 100 ms.

Interestingly, modern physiological models of the primary mamalian auditory system also considers the evolution of the energy distribution at longer time scales, \textit{i.e.} how the energy modulates through time for given frequencies.  Dau's model acount for frequency and rate of modulation. Shamma's model account for frequency, rate and bandwidth to fully describes the physiological evidence gathered by studying the auditory system of the ferret. Though, most of the numerical experiments done using this model did not fully demonstrated the usefullness of this third dimension.

Thus, by taking a signal processing view of the matter, one could consider that a first stage of the auditory system proceeds to a rather fixed set of convolutions across time and frequency, projecting the signal into a large set of descriptive features organized across frequency and rate of modulations.

What the higher level of the auditory cortex does remains largely unkown, but we can assume that the level of plasticity is very high, as any living entity has to adapt their percepts to a wide diversity of tasks.  Considering the lack of information about this stage, we will assume in this paper that this second step is rather opportunistic, in the sense that, from this large set of features, it aims at multiplexing in some way the ones that are the most relevant for the task at hand.

Considering not only the type of musical instrument but also the playing technique is interesting in that matter, as it invite us to consider the relation between the rate of modulation and timbre percepts. A notion that have not been extensively studied in the litterature. Compared to instrument class, the reference that is usually taken in timbre studies, it also permits to consider the notion of timbral similarity at finer grain of detail.

The contributions of the paper are as follows: 1) provide perceptual data that account for the perception of musical tones played with various playing techniques, 2), demonstrate that, for the gathered data, the type of playing technique plays an important role in the spontaneous judgments of similarity, 3) propose a perceptually motivated computational model of timbre perception that account well for the human judgments studied in this paper and 4) discusses the use of this computational model to \ipt similarity search in the usage scenario of \ipt database browsing.

\section{Experiment 1}\label{sec:xp1}

The dataset considered in this study has audio recording of 16 different  musical instruments () played with different playing techniques, which leads to 143 different couple of instrument / playing technique. For each couple, the pitch and intonation is varied leading to 25444 samples. The way the dataset is segmented is explained in more details in Appendix \ref{sec:dataset}.

Studies about perceptual similarity are plagued with a dimensional problem. When considering $n$ items, a complete exploration of the similarity space requires the filling of a $n^2$ matrix. Assuming a symmetric similarity (the similarity of A to B is equl to the similarity of B to A) reduces the number but not by much.

In an attempt to reduce the number of items, we decided to perform a selection experiment where the subject is asked to give his opinion on which \ipt is relevant to study. An \ipt is said to be relevant to study if it is likely to be associated to another \ipt. Interest is rated on a 7 ticks scale. The subject can listen to all the different nuance and pitch samples of each \ipt. The experiment is over when all the \ipt have been rated.

Rating examples are given :
\begin{itemize}
  \item One star: this \ipt is singular, it is not useful to compare it with another \ipt.
  \item Four stars: there is a proximity on one aspect of the sound between this \ipt and of an \ipt of another instrument, but this one is neither decisive nor obvious.
  \item Seven stars: there is a large similarity between this \ipt and an \ipt of another instrument, it is interesting to keep it.
\end{itemize}

Two musical composition professors of the CNSMDP, a renowned French school of music performed the test. All the couples with a grade higher than three are selected, bringing 78 \ipt to study.

\section{Experiment 2}\label{sec:xp2}

The second experiment aims at gathering spontaneous judgment of similarity. To do so, we chose a canonical free sorting task  for several reasons. First, it gives the ability to the subject to listen to the whole set of \ipt before performing the task. Second, it gives more freedom to the subject thus potentially more interest to perform the task than a lengthy series of triplet dissimilarity ratings.

The subjects are thus asked to organize the 78 \ipt displayed as grey dots on a two dimensional plane into groups by assigning colors to each dots according to their "acoustic similarity". The textual instructions provided to introduce the experiment was chosen to be deliberately vague in order to allow the subject to \ml{use its own representation of similarity}.

The \ipt can be listened to by hovering the mouse on the corresponding dot. The experiment is over when the sujbect assigned a color to all the dots. The initial location of the dots on the plane is randomized for each subject and can be changed during the experiment by the subject for convenience. Eventhough the location of the dots is recorded, this data is not considered during the analysis.

The experiment was available online for two months and requests to perform the test have been sent to internal mailing lists of the CNSMDP and international research mailing with a focus on audio and music processing. The experiment have been performed on a volountary basis without gratification.

\section{Analysis}\label{sec:analysis}

\begin{figure}
\center
\includegraphics[width = \textwidth]{figures/xp2example.png}
\caption{Spatial and group organization provided by subject 1.}
\label{fig:xp2display}
\end{figure}

31 subjects completed the experiment. An example of resulting organisation of \ipt is given on Figure \ref{fig:xp2display}.  The resulting organisation for each subject are available for reader's inspection using the full feature interface provided to the subjects for performing the experiment\footnote{https://mathieulagrange.github.io/paperSpontaneousSimilarity/demo}.

As discussed in the previous section, only the grouping given by the subjects using the color labels is considered to predict the spontaneous judgments of similarity among \ipt. The spatial organisation of the dots representing the \ipt could provide information about the similarity, but this would implicitely force the fact that the similarity space is two dimensional.

We therefore now only study the properties of the groupings performed by the subject prior to converting them into an overall similarity matrix.

Among the 20 colors available, the subjects used on average $10.2 \pm  4.1$ different colors to group \ipt. The distribution of the number of groups can be seen on Figure \ref{fig:xp2nbGroup}. The size of the groups is on average $7.7 \pm   7.2$. The distribution of the size of groups can be seen on Figure \ref{fig:xp2sizeGroup}. The large number of small groups indicates that many subjects considered that a few of the \ipt were very different from all the remaining \ipt.

\begin{figure}
\center
\includegraphics[width = \textwidth]{figures/nbc.png}
\caption{Histogram of the number of groups.}
\label{fig:xp2nbGroup}
\end{figure}

\begin{figure}
\center
\includegraphics[width = \textwidth]{figures/sbc.png}
\caption{Histogram of the size of groups.}
\label{fig:xp2sizeGroup}
\end{figure}

A grouping can be considered as a binary similarity measure, with $s(a, b) = 1$ if $a$ and $b$ belong to the same group, and $0$ otherwise. The Figure \ref{fig:xp2bsim} show the binary similarity matrix that correponds to the grouping displayed on Figure \ref{fig:xp2display}. By averaging the 31 binary similarity matrix, an overall floating point matrix is obtained that correponds to our measure of the spontaneous similarity between \ipt.

A cluster analysis is next performed using an agglomerative hierarchical clustering using the weighted average algorithm for computing distance between clusters. The resulting dendrogram is displayed on Figure \ref{fig:xp2sizeGroup}.

A partition of $n$ clusters can be constructed from the agglomerative hierarchical cluster tree by finding the smallest height at which a horizontal cut through the tree leaves $n$ clusters. In order to have a better understanding of the nature of the similarity space induced by the judgments of the subjects, a cluster analysis is now performed.

One question that arises is whether the similarity is influenced primarily by the instrument or by the playing technique. To answer this question, each partition provided by cutting the dendrogram at successive levels is compared to two reference partitions. One is the partition of the \ipt when labeled with the instrument used (I) and the other if the partition of the \ipt when labeled with the playing technique used (Pt).

There are many metrics to compare two partitions by evaluating their degree of correspondance. The normalized mutual information (NMI) is interesting as it do not require alignement of the groups labels between the two partitions prior correspondance analysis and the two partition are not required to have the same number of groups. It also by design bounded between 0 and 1, 1 meaning perfect match. Figure \ref{fig:clusters} shows the evolution of the NMI when comparing the partitions provided by the cluster analysis of the dendrogram produced using the human judgment of similarity and respectively the instrument partition ($j/I$) or the playing technique partition ($j/Pt$).

There is a clear gain of NMI when considering the playing technique which means that the subjects primarily used cues corresponding to the playing technique to organize the \ipt. Also, for both curves, a global trend of increase NMI with respect to the number of clusters in the $j$ partition can be observed. This is a well documented fact, which simply put, means that it is easier to match a given partition with more clusters at hand. To compensate for this bias, one can consider the NMI that would be achieved by comparing several random partitions of increasing number of clusters to the reference one, be it the instrument partition ($nullI$) or the playing technique partition ($nullPt$). The resulting averaged NMI for 100 randomly generated partitions are displayed on Figure \ref{fig:clusters}. Those curves are useful for normalizing $j/I$ and $j/Pt$ by simply substracting the so)called null NMIs:
\begin{equnarray}
  nj/I &=& j/I - nullI  \\
  nj/Pt &=& j/Pt - nullPt  \\
\end{equnarray}
This normalization is useful for identifying more easily the number of clusters for which the comparison is most relevant. As can be seen on Figure \ref{fig:clusters}, the number of clusters that leads to the highest NMI is 5 for the playing technique reference and 8 for the instrument reference, leading respectively to an NMI of $0.46$ and $0.21$. For the latter, a smaller number down to 6 do not decrease the NMI by much, a fact that will be latter discussed in more details. Even by considering the null normalization, the matching level is much higher for the playing technique refereence.

\ml{need the dendrogram}

Once the number of clusters that lead to a partition that optimally match the reference, be it the instrument or playing technique labels are set, we can now study in more detailes the correspondance between those partitions and the reference ones. For this purpose, Figure \ref{fig:gi} displays two partitions. At the bottom is the partition that corresponds to the instruments labels, whose color code is given by the color bar on the right. The abbreviations used for the instruments are detailed in Section \ref{sec:dataset}. The ordering of \ipt on the horizontal axis is the one given by the cluster analysis, see the dendrogram on Figure \ref{fig:dendrogram}. The partition on top is the result of the clustering with 8 clusters. For visualization purposes, we arbitrarily set colors to clusters of this partition to the color code of the instrument label that is most often encountered in this cluster. If some some occurs equally often, the instrument of the first \ipt of the cluster is chosen. The emerging instrument classes are: Saxophone, Clarinet, Trumpet, Flute and Cello.

Though, even with an optimal number of clusters, we see that the clusters have  a low purity, \textit{i.e.} a very diverse set of instruments labels for the \ipt they belong to. Also, some clusters could be merged without stong changes in the overall organisation. This probably explains the small changes of NMI when reducing the number of clusters, see Figure \ref{fig:clusters}.

As can be seen on Figure \ref{fig:gm}, the correspondance between the playing technique reference and the resulting partition with 5 clusters is much higher. Most clusters have a high level of purity and for groups emerge : pizzicato, slap, ordinario, and ponticello.

To conclude this section, the process of gathering data about the perceptual similarity of \ipt is found to be successful. The main organizationtal factor is found to be the playing technique.

\begin{figure}
\center
\includegraphics[width = \textwidth]{figures/clusterAnalysis.png}
\caption{Cluster analysis of the similarity judgments. $j/I$ and $j/Pt$ compare the partition obtained using the similarity judgments clustered using a growing number of clusters versus respectively the instrument partition and the playing technique partition. Normalizing factors $nullI$ and $nullPt$ are used to obtain normalized measures: $nj/I$ and $nj/Pt$. \ml{change legend}}
\label{fig:clusters}
\end{figure}

\begin{figure}
\center
\includegraphics[width = \textwidth]{figures/groupModes.png}
\caption{Clustering \ipt using average perceptual similarity into 5 clusters (top) and playing technique labels (top). The color chosen to display each clusters on top corresponds to the playing technique that is dominant in this cluster.\ml{change colormap}}
\label{fig:gm}
\end{figure}

\begin{figure}
\center
\includegraphics[width = \textwidth]{figures/groupInstruments.png}
\caption{Clustering \ipt using average perceptual similarity into 8 clusters (a) and instrument reference labels (b). The color chosen to display the groups (on top) corresponds  to the instrument that is dominant in this group.}
\label{fig:gi}
\end{figure}

\section{Model}\label{sec:model}

Once the perceptual data is gathered and controled, our aim is to evaluate the ability of the proposed perceptual model that is now introduced in more details. The model is composed of two processing stages, respectively called the primary and the secondary stage, see Figure \ref{fig:model}. The primary stage is designed to rely on very few meta parameters set arbitrarily by the experimenter to mimic the early stages of the mamalian auditory system. On contrary, the secondary stage rely largely on the data it has to model and in this respect mimic the high plasticity of the later stages of the mamalian auditory system.

\begin{figure}
\center
\includegraphics[width = \textwidth]{figures/groupInstruments.png}
\caption{Computational model \ml{TODO}}
\label{fig:model}
\end{figure}

\subsection{Primary stage}

In order to model the ealry stages of the mamalian auditory system, the time scattering transform, obtained by applying a one dimensional
wavelet transform in time to a time-frequency wavelet scalogram. This transform thus encodes well modulations over time of energy distributed over frequency, dimensions that are shown to be perceptually important.

This model is chosen over more complex alternatives for the sake of simplicity. Deep convolutional networks could have been considered, but the learning stage inherent in this kind of methods did not seem relevant for this study. On contrary, the scattering transform can be conveniently parametrized to match the perceptual capabilities of the auditory system. Those parameters do not change significantly among subjects. Also, more complex versions of the scattering like the joint time-frequency scattering \cite{waspaa} and the spiral scattering are not considered in order to achieve a projection of reasonable dimensionality and for lack of evidence that those dimensions are mandatory to model the primary stage of the auditory system \cite{dau}.

\subsection{Secondary stage}

The secondary stage aims at providing a metric space that closely match the one resulting from the data gathering by experiment 2. For the reason, it involves a learning based process block that weight the features outputed by the primary stage.

Indeed, if the properties of the first processing stages of the human auditory system can reasonably be considered as similar for every subjects despite potential hearing losses for some, the later ones is expected to vary more drastically due to his education, tastes and experiences with stimuli similar to the \ipt that are considered in this study.



\section{Validation}\label{sec:validation}

\subsection{Methods}

\subsection{Data}

\subsection{Results}

For the sake of reproducibility, the code of the proposed method and the experimental protocol is available\footnote{https://github.com/mathieulagrange/paperSpontaneousSimilarity}. For convenience, the features (scattering and mfccs) are alos provided, but the software can be used to process from scratch the SOL database.

\section{Discussion}\label{sec:discussion}

Thanks to the digitization capabilities available, gathering large amount of audio recordings of \ipt is achievable and several databases are now available on the market. Browsing of such databases can be achieved by keyword based search. It requires that 1) the database is fully annotated with a precise ontology of instruments and playing techniques, 2) that the user has a precise understanding of this ontology, and 3) that the user a precise idea of the kind of timbre he is looking to find the \ipt he is searching for.

An interesting alternative type of search is the similarity content based search. Being solely based on sound, this kind of search can be used when querying by example. The user provides an example of the target sound to find \ipt that match the timbre description of the query, be it an onomatepea, a tone from another instrument or any kind of environmental sound.

The computational model proposed in this paper is well suited for this kind of search, as it can model complex perceptual timbre spaces and can scale to large databases. Once the features and the projection matrix are computed, the similarity computation is quite effective and can be further enhanced by considering fast similarity search scheme such as locally sensitive least square hashing \cite{lsh}. The secondary stage being learnt, it can easily adapt to user tastes and usage history by reweighting the scattering features for improved search relevance.

% For bibtex users:
\bibliography{bib}

\section{Dataset}

distribution of instruments and modes

\subsection{Naming}

abbreviations 4 instruments and modes.



% For non bibtex users:
%\begin{thebibliography}{citations}
%
%\bibitem {Author:00}
%E. Author.
%``The Title of the Conference Paper,''
%{\it Proceedings of the International Symposium
%on Music Information Retrieval}, pp.~000--111, 2000.
%
%\bibitem{Someone:10}
%A. Someone, B. Someone, and C. Someone.
%``The Title of the Journal Paper,''
%{\it Journal of New Music Research},
%Vol.~A, No.~B, pp.~111--222, 2010.
%
%\bibitem{Someone:04} X. Someone and Y. Someone. {\it Title of the Book},
%    Editorial Acme, Porto, 2012.
%
%\end{thebibliography}

\end{document}
